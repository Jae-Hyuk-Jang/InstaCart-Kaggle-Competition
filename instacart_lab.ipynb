{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMilkBiVfLgSbagYWyecTPl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wlBfW75ytLZL"},"outputs":[],"source":["# @title ğŸ”— 0. Google Drive ë§ˆìš´íŠ¸ & ê²½ë¡œ ì„¤ì •\n","from google.colab import drive\n","import os, gc, warnings\n","\n","drive.mount('/content/drive')   # ìµœì´ˆ 1íšŒ ì¸ì¦ í•„ìš”\n","ROOT_DIR  = '/content/drive/MyDrive/instaCart_data'\n","os.makedirs(ROOT_DIR, exist_ok=True)\n","gc.enable()"]},{"cell_type":"code","source":["# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# @title 1. ğŸ“¦ PyG (CPU) & ê¸°íƒ€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜  â€• Torch 2.6.0 + CPU LightGBM Â· XGBoost Â· CatBoost (NumPy-2.x í˜¸í™˜íŒ)\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","!pip install -q --upgrade pip setuptools wheel\n","\n","!pip -q install lightgbm==4.3.0 xgboost==2.0.3  category_encoders pytorch_tabnet optuna\n","# 1) PyG core 3ì¢… + pyg-lib  â–¶  torch-2.6.0+cpu wheel index\n","WHEEL_URL=\"https://data.pyg.org/whl/torch-2.6.0+cpu.html\"\n","!pip -q install pyg-lib torch-scatter torch-sparse -f $WHEEL_URL\n","!pip -q install torch-geometric          # ì´ê±´ ê³µì‹ PyPI (CPU ë¹Œë“œ)\n","# 2) ì„ íƒ ë¼ì´ë¸ŒëŸ¬ë¦¬\n","!pip -q install torch-geometric-temporal faiss-cpu neo4j ipywidgets neomodel tqdm jedi>=0.18.0 catboost\n","# 3) LightGBM Â· XGBoost Â· CatBoost (NumPy-2.x í˜¸í™˜íŒ)\n","!pip -q install lightgbm==4.3.0 xgboost==2.0.3  category_encoders pytorch_tabnet optuna\n","# 4) LightGBM update\n","!pip install --upgrade lightgbm\n","import lightgbm, numpy\n","print(lightgbm.__version__, numpy.__version__)  # 4.3.x  2.0.x"],"metadata":{"id":"PgmkMCKStTar"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===============================================================\n","#  @title 2. Stacking Feature Engineering â€“ ë§ˆì§€ë§‰ ì£¼ë¬¸ ì˜ˆì¸¡ (userÃ—product ë‹¨ì¼ row)\n","# ===============================================================\n","import gc, warnings\n","import numpy as np, pandas as pd\n","from pathlib import Path\n","warnings.filterwarnings('ignore')\n","\n","ROOT_DIR = '/content/drive/MyDrive/instaCart_data'\n","OUT_DIR  = '/content/drive/MyDrive/instaCart_data/feature2'\n","\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# util\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","DTYPES = {\n","    'order_id': np.int32, 'user_id': np.int32, 'product_id': np.uint16,\n","    'aisle_id': np.uint16, 'department_id': np.uint8,\n","    'order_number': np.uint8, 'order_dow': np.uint8, 'order_hour_of_day': np.uint8,\n","    'days_since_prior_order': np.float32, 'add_to_cart_order': np.uint8,\n","    'reordered': np.uint8\n","}\n","def mem_crunch(df, to_float16=False):\n","    for c in df.columns:\n","        if df[c].dtype == 'float64':\n","            df[c] = df[c].astype('float16' if to_float16 else 'float32')\n","        elif df[c].dtype == 'int64':\n","            df[c] = df[c].astype('int32')\n","    return df\n","\n","def max_streak(arr):\n","    m = cur = 0\n","    for v in arr:\n","        cur = cur + 1 if v else 0\n","        m   = max(m, cur)\n","    return m\n","\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 1. CSV ë¡œë“œ\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","aisles    = pd.read_csv(f'{ROOT_DIR}/aisles.csv')\n","depart    = pd.read_csv(f'{ROOT_DIR}/departments.csv')\n","orders    = pd.read_csv(f'{ROOT_DIR}/orders.csv', dtype=DTYPES)\n","prior     = pd.read_csv(f'{ROOT_DIR}/order_products__prior.csv', dtype=DTYPES)\n","train_lbl = pd.read_csv(f'{ROOT_DIR}/order_products__train.csv', dtype=DTYPES)\n","products  = pd.read_csv(f'{ROOT_DIR}/products.csv')\n","\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 2. ì œí’ˆÂ·ì¹´í…Œê³ ë¦¬ ë©”íƒ€\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","prods = (\n","    products.merge(aisles, on='aisle_id')\n","            .merge(depart, on='department_id')\n","            [['product_id','aisle_id','department_id']]\n","            .astype({'product_id':np.uint16,'aisle_id':np.uint16,'department_id':np.uint8})\n",")\n","del aisles, depart, products\n","gc.collect()\n","\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 3. â€œì…ë ¥ vs ì˜ˆì¸¡â€ ì£¼ë¬¸ êµ¬ë¶„:\n","#     ì…ë ¥ = 1~(max_ordâˆ’2), íƒ€ê²Ÿ = ì‹¤ì œ ë§ˆì§€ë§‰ ì£¼ë¬¸(eval_set=='train')\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","orders['max_ord']   = orders.groupby('user_id')['order_number'].transform('max')\n","orders['is_input']  = orders['order_number'] <= orders['max_ord'] - 2    # 1~maxâˆ’2\n","orders['is_target'] = orders['eval_set'] == 'train'                     # ì‹¤ì œ ë§ˆì§€ë§‰\n","\n","# ë§ˆì§€ë§‰ ì£¼ë¬¸ë§Œ ë½‘ì•„ train_order_id ë¡œ ì‚¬ìš©\n","orders_target = (\n","    orders.loc[orders.is_target, ['user_id','order_id']]\n","          .rename(columns={'order_id':'train_order_id'})\n",")\n","\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 4. prior_orders ë° orders_meta ì¤€ë¹„\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","orders_prior = orders.loc[orders.is_input,\n","    ['order_id','user_id','order_number','order_dow','order_hour_of_day','days_since_prior_order']\n","]\n","prior_orders = mem_crunch(prior.merge(orders_prior, on='order_id', how='inner'))\n","orders_meta  = mem_crunch(\n","    orders[['order_id','order_dow','order_hour_of_day','days_since_prior_order']]\n",")\n","\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 5. User & Product íŠ¹ì„± ì§‘ê³„\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","print('ğŸ§®  aggregating â€¦')\n","# â€” User features\n","user_feats = mem_crunch(\n","    prior_orders.groupby('user_id').agg(\n","        u_total_orders      = ('order_number','max'),\n","        u_avg_days_between  = ('days_since_prior_order','mean'),\n","        u_reorder_ratio     = ('reordered','mean'),\n","        u_distinct_products = ('product_id','nunique'),\n","        u_avg_basket        = ('product_id','size'),\n","        u_avg_order_hour    = ('order_hour_of_day','mean'),\n","        u_avg_order_dow     = ('order_dow','mean')\n","    ).reset_index(),\n","    to_float16=True\n",")\n","# â€” Product features\n","prod_feats = mem_crunch(\n","    prior_orders.groupby('product_id').agg(\n","        p_total_orders   = ('order_id','size'),\n","        p_total_reorders = ('reordered','sum'),\n","        p_reorder_rate   = ('reordered','mean'),\n","        p_avg_cart_pos   = ('add_to_cart_order','mean')\n","    ).reset_index()\n","      .merge(prods, on='product_id', how='left'),\n","    to_float16=True\n",")\n","# Dept & Aisle reorder rates, popularity rank\n","dept_rate  = prior_orders.merge(prods[['product_id','department_id']])\\\n","                         .groupby('department_id').reordered.mean().rename('dept_reorder_rate')\n","aisle_rate = prior_orders.merge(prods[['product_id','aisle_id']])\\\n","                         .groupby('aisle_id').reordered.mean().rename('aisle_reorder_rate')\n","pop_rank   = np.log1p(prior_orders.product_id.value_counts()\\\n","                       .rank(ascending=False,method='min')).rename('p_pop_rank')\n","prod_feats = mem_crunch(\n","    prod_feats.merge(dept_rate , on='department_id', how='left')\n","              .merge(aisle_rate, on='aisle_id',      how='left')\n","              .merge(pop_rank.to_frame(), on='product_id', how='left'),\n","    to_float16=True\n",")\n","\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 6. UserÃ—Product ì§‘ê³„ (up_feats)\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","up_base = prior_orders.groupby(['user_id','product_id']).agg(\n","    up_orders       = ('order_id','size'),\n","    up_last_order   = ('order_number','max'),\n","    up_first_order  = ('order_number','min'),\n","    up_avg_cart_pos = ('add_to_cart_order','mean')\n",").reset_index()\n","up_base['u_total_orders']       = up_base.user_id.map(user_feats.set_index('user_id')['u_total_orders'])\n","up_base['up_days_since_last']   = (up_base.u_total_orders - up_base.up_last_order + 1).astype(np.uint8)\n","up_base['up_orders_since_last'] =  up_base.u_total_orders - up_base.up_last_order + 1\n","up_base['up_order_rate']        =  up_base.up_orders / up_base.u_total_orders\n","\n","# ìµœê·¼ 5íšŒ ë“±ì¥ ì¹´ìš´íŠ¸\n","prior_orders['rev_order'] = prior_orders.groupby('user_id')['order_number']\\\n","                                        .transform('max') - prior_orders['order_number'] + 1\n","last5 = (prior_orders[prior_orders.rev_order<=5]\n","         .groupby(['user_id','product_id']).size()\n","         .rename('up_last5_cnt').reset_index())\n","up_base = up_base.merge(last5, on=['user_id','product_id'], how='left').fillna({'up_last5_cnt':0})\n","\n","# ì—°ì† ì¬êµ¬ë§¤ streak\n","valid_pairs = up_base.query('up_orders>=3')[['user_id','product_id']]\n","sub = (prior_orders.merge(valid_pairs, on=['user_id','product_id'], how='inner')\n","       .sort_values(['user_id','product_id','order_number'])\n","       [['user_id','product_id','reordered']].to_numpy(np.int32))\n","keys = (sub[:,0].astype(np.int64)<<20) + sub[:,1]\n","segments = np.where(np.r_[True, keys[1:]!=keys[:-1]])[0]\n","user_list, prod_list, streaks = [], [], []\n","for i in range(len(segments)):\n","    a,b = segments[i], segments[i+1] if i+1<len(segments) else len(sub)\n","    user_list.append(sub[b-1,0]); prod_list.append(sub[b-1,1])\n","    streaks.append(max_streak(sub[a:b,2]))\n","up_streak = pd.DataFrame({'user_id':user_list,'product_id':prod_list,'up_max_reorder_streak':streaks})\n","up_feats = mem_crunch(\n","    up_base.merge(up_streak, on=['user_id','product_id'], how='left')\n","           .fillna({'up_max_reorder_streak':0}),\n","    to_float16=True\n",")\n","\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 7. í›„ë³´ & train_order_id(ë§ˆì§€ë§‰ ì£¼ë¬¸) ë¶€ì°©\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","cand_pool = (\n","    up_feats.query('up_orders>=3 or up_last5_cnt>0')[['user_id','product_id']]\n","            .merge(orders_target, on='user_id', how='left')\n",")\n","print('# cand_pool rows :', len(cand_pool))\n","\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 8. ë¼ë²¨ ë³‘í•© (ë§ˆì§€ë§‰ ì£¼ë¬¸ GT)\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","train_df = (\n","    cand_pool\n","    .merge(\n","        train_lbl[['order_id','product_id','reordered']],\n","        left_on = ['train_order_id','product_id'],\n","        right_on= ['order_id','product_id'],\n","        how='left'\n","    )\n","    .fillna({'reordered':0})\n","    .astype({'reordered':'uint8'})\n",")\n","# ì¤‘ë³µëœ ì˜¤ë¥¸ìª½ order_id ì œê±°, train_order_id â†’ order_id ë¡œ rename\n","train_df = (train_df\n","            .drop(columns=['order_id'])\n","            .rename(columns={'train_order_id':'order_id'}))\n","\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 9. ìµœì¢… í”¼ì²˜ ë¹Œë“œ\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","def build_features(df):\n","    return mem_crunch(\n","        df.merge(up_feats   , on=['user_id','product_id'], how='left')\n","          .merge(user_feats , on='user_id'             , how='left')\n","          .merge(prod_feats , on='product_id'          , how='left')\n","          .merge(orders_meta, on='order_id'            , how='left')\n","          .fillna(0),\n","        to_float16=True\n","    )\n","\n","X_train = build_features(train_df.drop(columns=['reordered']))\n","y_train = train_df['reordered'].values.astype('uint8')\n","X_test  = build_features(cand_pool.rename(columns={'train_order_id':'order_id'}))\n","\n","print('âœ… Shapes â†’',\n","      f'X_train {X_train.shape}, y_train {y_train.shape}, X_test {X_test.shape}')\n","print('y_train ë¶„í¬:', np.unique(y_train, return_counts=True))\n","\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 10. Export CSV\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n","user_feats.to_csv(f'{OUT_DIR}/user_feats.csv'  , index=False)\n","prod_feats.to_csv(f'{OUT_DIR}/product_feats.csv', index=False)\n","up_feats  .to_csv(f'{OUT_DIR}/up_feats.csv'    , index=False)\n","X_train   .to_csv(f'{OUT_DIR}/X_train.csv'     , index=False)\n","pd.DataFrame({'reordered':y_train}).to_csv(f'{OUT_DIR}/y_train.csv', index=False)\n","X_test    .to_csv(f'{OUT_DIR}/X_test.csv'      , index=False)\n","\n","print('ğŸ“¦ feature tables saved to', OUT_DIR)"],"metadata":{"id":"8R0jmfgxtW93"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===============================================================\n","#  @title 3. RAM-safe 3-fold OOF Stacking\n","#         (no down-sample, Optuna HP-search, AUC-based, +CatBoost+MLP)\n","# ===============================================================\n","import gc, warnings, joblib, os, numpy as np, pandas as pd, torch\n","from pathlib import Path\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import (\n","    accuracy_score, precision_score, recall_score,\n","    f1_score, roc_auc_score\n",")\n","import lightgbm as lgb, xgboost as xgb, catboost as cb, optuna\n","from sklearn.neural_network import MLPClassifier\n","\n","warnings.filterwarnings('ignore')\n","\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 0. Paths & Constants\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","ROOT        = '/content/drive/MyDrive/instaCart_data'\n","FEATURE_DIR = f'{ROOT}/feature2'\n","STACK_DIR   = f'{ROOT}/stack2'\n","Path(STACK_DIR).mkdir(parents=True, exist_ok=True)\n","\n","NFOLDS, RS  = 3, 2025\n","THR_SCAN    = np.arange(.15, .35, .01)\n","\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 1. í”¼ì²˜ ë¶ˆëŸ¬ì˜¤ê¸° (ID ì»¬ëŸ¼ ì•ˆì „ ì²˜ë¦¬)\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","id_cols  = ['order_id','user_id','product_id']\n","cat_cols = ['order_dow','order_hour_of_day']\n","\n","# (1) ìƒ˜í”Œ ì»¬ëŸ¼ ì½ì–´ì„œ, IDÂ·ë²”ì£¼í˜• ì œì™¸í•œ dtype ì‚¬ì „ ìƒì„±\n","sample_cols = pd.read_csv(f'{FEATURE_DIR}/X_train.csv', nrows=0).columns\n","dtypes_noid = {\n","    c: 'float32'\n","    for c in sample_cols\n","    if c not in id_cols + cat_cols\n","}\n","dtypes_noid.update({c: 'uint8' for c in cat_cols})\n","\n","# (2) CSV ë¶ˆëŸ¬ì˜¤ê¸° (ID ì»¬ëŸ¼ì€ ë‚˜ì¤‘ì— ì²˜ë¦¬)\n","X_full = pd.read_csv(\n","    f'{FEATURE_DIR}/X_train.csv',\n","    dtype=dtypes_noid,\n","    engine='pyarrow',\n","    na_values=['', 'NA', 'nan']\n",")\n","y_full = pd.read_csv(f'{FEATURE_DIR}/y_train.csv')['reordered'].astype('uint8').values\n","X_test = pd.read_csv(\n","    f'{FEATURE_DIR}/X_test.csv',\n","    dtype=dtypes_noid,\n","    engine='pyarrow',\n","    na_values=['', 'NA', 'nan']\n",")\n","\n","# (3) ID ì»¬ëŸ¼ì„ ìˆ«ìë¡œ ë³€í™˜í•˜ë˜, ë¹„ì •ìƒ ê°’ì€ NaNâ†’-1 ì²˜ë¦¬\n","for df in (X_full, X_test):\n","    for col in id_cols:\n","        df[col] = pd.to_numeric(df[col], errors='coerce') \\\n","                     .replace([np.inf, -np.inf], np.nan) \\\n","                     .fillna(-1) \\\n","                     .astype('int32')\n","\n","# (4) ëª¨ë¸ ì…ë ¥ìš©ìœ¼ë¡œ ID ì»¬ëŸ¼ ì œì™¸\n","X_full_enc = X_full.drop(columns=id_cols)\n","X_test_enc = X_test .drop(columns=id_cols)\n","del X_full, X_test\n","gc.collect()\n","\n","print(f'Loaded: {X_full_enc.shape} rows, RAM â‰ˆ {X_full_enc.memory_usage().sum()/1e9:.2f} GB')\n","\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 2. Optuna HP Search (AUC objective) for LGB, XGB\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","def lgb_obj(trial):\n","    params = dict(\n","        boosting_type='gbdt', objective='binary', device_type='gpu', metric='auc',\n","        learning_rate=trial.suggest_float('lr', 1e-2, 1e-1, log=True),\n","        num_leaves=trial.suggest_int('num_leaves', 31, 255),\n","        max_depth=trial.suggest_int('max_depth', 4, 12),\n","        feature_fraction=trial.suggest_float('feature_fraction', .5, .9),\n","        bagging_fraction=trial.suggest_float('bagging_fraction', .5, .9),\n","        bagging_freq=trial.suggest_int('bagging_freq', 1, 10),\n","        lambda_l1=trial.suggest_float('l1', 1e-3, 1, log=True),\n","        lambda_l2=trial.suggest_float('l2', 1e-3, 1, log=True),\n","        max_bin=127, min_data_in_leaf=20, force_row_wise=True, verbosity=-1\n","    )\n","    skf = StratifiedKFold(NFOLDS, shuffle=True, random_state=RS)\n","    aucs = []\n","    for tr, va in skf.split(X_full_enc, y_full):\n","        dtr = lgb.Dataset(X_full_enc.iloc[tr], label=y_full[tr])\n","        dvl = lgb.Dataset(X_full_enc.iloc[va], label=y_full[va])\n","        try:\n","            bst = lgb.train(\n","                params, dtr,\n","                num_boost_round=500,\n","                valid_sets=[dvl],\n","                callbacks=[lgb.early_stopping(50)]\n","            )\n","            preds = bst.predict(X_full_enc.iloc[va], num_iteration=bst.best_iteration)\n","            auc = roc_auc_score(y_full[va], preds)\n","            aucs.append(0.0 if np.isnan(auc) else auc)\n","        except Exception:\n","            # ë¶„í•  ì‹¤íŒ¨ ë“±ìœ¼ë¡œ ì—ëŸ¬ ë°œìƒ ì‹œ, í•´ë‹¹ FoldëŠ” 0.0 ì ìˆ˜ë¡œ ê°„ì£¼\n","            aucs.append(0.0)\n","    return float(np.mean(aucs))\n","\n","\n","def xgb_obj(trial):\n","    params = dict(\n","        objective='binary:logistic', tree_method='gpu_hist', eval_metric='auc',\n","        learning_rate=trial.suggest_float('lr', 1e-2, 1e-1, log=True),\n","        max_depth=trial.suggest_int('max_depth', 4, 10),\n","        min_child_weight=trial.suggest_int('min_child', 1, 10),\n","        subsample=trial.suggest_float('subsample', .5, .9),\n","        colsample_bytree=trial.suggest_float('col_bt', .5, .9),\n","        lambda_l1=trial.suggest_float('l1', 1e-3, 1, log=True),\n","        lambda_l2=trial.suggest_float('l2', 1e-3, 1, log=True),\n","        n_estimators=500, random_state=RS\n","    )\n","    skf = StratifiedKFold(NFOLDS, shuffle=True, random_state=RS)\n","    aucs = []\n","    for tr, va in skf.split(X_full_enc, y_full):\n","        try:\n","            m = xgb.XGBClassifier(**params)\n","            m.fit(\n","                X_full_enc.iloc[tr], y_full[tr],\n","                eval_set=[(X_full_enc.iloc[va], y_full[va])],\n","                early_stopping_rounds=50, verbose=False\n","            )\n","            preds = m.predict_proba(X_full_enc.iloc[va])[:, 1]\n","            auc = roc_auc_score(y_full[va], preds)\n","            aucs.append(0.0 if np.isnan(auc) else auc)\n","        except Exception:\n","            # ì—ëŸ¬ ë°œìƒ ì‹œ, í•´ë‹¹ FoldëŠ” 0.0 ì ìˆ˜ë¡œ ê°„ì£¼\n","            aucs.append(0.0)\n","    return float(np.mean(aucs))\n","\n","print('Optuna LGB...')\n","study_lgb = optuna.create_study(direction='maximize',\n","                                sampler=optuna.samplers.TPESampler(seed=RS))\n","study_lgb.optimize(lgb_obj, n_trials=20)\n","BEST_LGB = study_lgb.best_trial.params | {\n","    'boosting_type':'gbdt','objective':'binary','device_type':'gpu','metric':'auc',\n","    'max_bin':127,'min_data_in_leaf':20,'force_row_wise':True,'verbosity':-1\n","}\n","print('LGB params', BEST_LGB)\n","\n","print('Optuna XGB...')\n","study_xgb = optuna.create_study(direction='maximize',\n","                                sampler=optuna.samplers.TPESampler(seed=RS))\n","study_xgb.optimize(xgb_obj, n_trials=20)\n","BEST_XGB = study_xgb.best_trial.params | {\n","    'objective':'binary:logistic','tree_method':'gpu_hist',\n","    'eval_metric':'auc','n_estimators':800,'random_state':RS\n","}\n","print('XGB params', BEST_XGB)\n","\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 3. OOF training: LGB, XGB, CatBoost, MLP\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","kf = StratifiedKFold(NFOLDS, shuffle=True, random_state=RS)\n","oof_lgb = np.zeros_like(y_full, dtype=float)\n","oof_xgb = np.zeros_like(y_full, dtype=float)\n","oof_cat = np.zeros_like(y_full, dtype=float)\n","oof_mlp = np.zeros_like(y_full, dtype=float)\n","\n","for fold, (tr, va) in enumerate(kf.split(X_full_enc, y_full), 1):\n","    print(f'Fold {fold}/{NFOLDS}')\n","    # LightGBM\n","    dtr = lgb.Dataset(X_full_enc.iloc[tr], label=y_full[tr])\n","    dvl = lgb.Dataset(X_full_enc.iloc[va], label=y_full[va])\n","    m1 = lgb.train(\n","        BEST_LGB, dtr, num_boost_round=1000,\n","        valid_sets=[dvl], callbacks=[lgb.early_stopping(100)]\n","    )\n","    oof_lgb[va] = m1.predict(X_full_enc.iloc[va], num_iteration=m1.best_iteration)\n","    joblib.dump(m1, f'{STACK_DIR}/lgb_fold{fold}.pkl')\n","\n","    # XGBoost\n","    m2 = xgb.XGBClassifier(**BEST_XGB)\n","    m2.fit(\n","        X_full_enc.iloc[tr], y_full[tr],\n","        eval_set=[(X_full_enc.iloc[va], y_full[va])],\n","        early_stopping_rounds=100, verbose=False\n","    )\n","    oof_xgb[va] = m2.predict_proba(X_full_enc.iloc[va])[:,1]\n","    joblib.dump(m2, f'{STACK_DIR}/xgb_fold{fold}.pkl')\n","\n","    # CatBoost\n","    m3 = cb.CatBoostClassifier(\n","        iterations=1000, learning_rate=0.05, depth=6,\n","        task_type='GPU', eval_metric='AUC',\n","        early_stopping_rounds=100, verbose=False\n","    )\n","    m3.fit(X_full_enc.iloc[tr], y_full[tr],\n","           eval_set=(X_full_enc.iloc[va], y_full[va]))\n","    oof_cat[va] = m3.predict_proba(X_full_enc.iloc[va])[:,1]\n","    joblib.dump(m3, f'{STACK_DIR}/cat_fold{fold}.pkl')\n","\n","    # MLP\n","    m4 = MLPClassifier(\n","        hidden_layer_sizes=(128,64),\n","        learning_rate_init=1e-3,\n","        max_iter=200,\n","        random_state=RS,\n","        verbose=False\n","    )\n","    m4.fit(X_full_enc.iloc[tr], y_full[tr])\n","    oof_mlp[va] = m4.predict_proba(X_full_enc.iloc[va])[:,1]\n","    joblib.dump(m4, f'{STACK_DIR}/mlp_fold{fold}.pkl')\n","\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 4. Blend & threshold ìµœì í™”\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","blend = (oof_lgb + oof_xgb + oof_cat + oof_mlp) / 4\n","best_thr = max(THR_SCAN, key=lambda t: f1_score(y_full, (blend>=t).astype(int)))\n","\n","# OOF ë©”íƒ€ ì €ì¥\n","np.savez(\n","    f'{STACK_DIR}/oof_meta.npz',\n","    oof_lgb=oof_lgb,\n","    oof_xgb=oof_xgb,\n","    oof_cat=oof_cat,\n","    oof_mlp=oof_mlp,\n","    best_thr=best_thr\n",")\n","\n","# ê°œë³„ & blend F1 ì ìˆ˜\n","print(f\"\\nğŸ“ best_thr = {best_thr:.4f} | \"\n","      f\"F1(LGB) = {f1_score(y_full, (oof_lgb>=best_thr).astype(int)):.4f} | \"\n","      f\"F1(XGB) = {f1_score(y_full, (oof_xgb>=best_thr).astype(int)):.4f} | \"\n","      f\"F1(CAT) = {f1_score(y_full, (oof_cat>=best_thr).astype(int)):.4f} | \"\n","      f\"F1(MLP) = {f1_score(y_full, (oof_mlp>=best_thr).astype(int)):.4f} | \"\n","      f\"F1(blend) = {f1_score(y_full, (blend>=best_thr).astype(int)):.4f}\")\n","\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 5. OOF Performance ì¶œë ¥ (Acc, Prec, Recall, F1, AUC)\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","oof_pred = (blend >= best_thr).astype(int)\n","print(\"\\nâœ… OOF Performance â†’ \"\n","      f\"Acc={accuracy_score(y_full, oof_pred):.4f}, \"\n","      f\"Prec={precision_score(y_full, oof_pred, zero_division=0):.4f}, \"\n","      f\"Recall={recall_score(y_full, oof_pred):.4f}, \"\n","      f\"F1={f1_score(y_full, oof_pred):.4f}, \"\n","      f\"AUC={roc_auc_score(y_full, blend):.4f}\")\n","\n","print('âœ… artifacts saved â†’', STACK_DIR)"],"metadata":{"id":"tj6Fk8p2tzyc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===============================================================\n","#  @title 4. full-inference using 3-fold models (LGB, XGB, CatBoost, MLP)\n","# ===============================================================\n","import gc, joblib, numpy as np, pandas as pd, warnings\n","from pathlib import Path\n","from sklearn.metrics import (\n","    accuracy_score, precision_score, recall_score,\n","    f1_score, roc_auc_score\n",")\n","warnings.filterwarnings('ignore')\n","\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â¶ ê²½ë¡œ & ë©”íƒ€ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","STACK_DIR   = '/content/drive/MyDrive/instaCart_data/stack2'\n","FEATURE_DIR = '/content/drive/MyDrive/instaCart_data/feature2'\n","OUT_DIR     = STACK_DIR\n","Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n","\n","# â€” OOF ë©”íƒ€\n","meta     = np.load(f'{STACK_DIR}/oof_meta.npz', allow_pickle=True)\n","oof_lgb  = meta['oof_lgb']\n","oof_xgb  = meta['oof_xgb']\n","oof_cat  = meta['oof_cat']\n","oof_mlp  = meta['oof_mlp']\n","best_thr = float(meta['best_thr'])\n","print('â„¹ï¸ best_thr =', best_thr)\n","\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â· OOF ì„±ëŠ¥ í™•ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# y_train ë¡œë“œ\n","y_tr = pd.read_csv(f'{FEATURE_DIR}/y_train.csv')['reordered'].astype('uint8').values\n","blend_oof = (oof_lgb + oof_xgb + oof_cat + oof_mlp) / 4\n","pred_oof  = (blend_oof >= best_thr).astype('uint8')\n","print(\n","    f\"OOF â†’ Acc={accuracy_score(y_tr,pred_oof):.4f} | \"\n","    f\"Prec={precision_score(y_tr,pred_oof,zero_division=0):.4f} | \"\n","    f\"Recall={recall_score(y_tr,pred_oof):.4f} | \"\n","    f\"F1={f1_score(y_tr,pred_oof):.4f} | \"\n","    f\"AUC={roc_auc_score(y_tr,blend_oof):.4f}\"\n",")\n","\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â¸ í…ŒìŠ¤íŠ¸ í”¼ì²˜ ë¶ˆëŸ¬ì˜¤ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","id_cols  = ['order_id','user_id','product_id']\n","cat_cols = ['order_dow','order_hour_of_day']\n","\n","# (1) dtype ì‚¬ì „ ì¤€ë¹„\n","sample = pd.read_csv(f'{FEATURE_DIR}/X_train.csv', nrows=0)\n","dtypes = {c:'float32' for c in sample.columns if c not in id_cols+cat_cols}\n","dtypes.update({c:'uint8' for c in cat_cols})\n","\n","# (2) CSV ë¡œë“œ\n","X_test = pd.read_csv(\n","    f'{FEATURE_DIR}/X_test.csv',\n","    dtype=dtypes, engine='pyarrow',\n","    na_values=['','NA','nan']\n",")\n","# (3) ID ì»¬ëŸ¼ ì•ˆì „ ì²˜ë¦¬\n","for col in id_cols:\n","    X_test[col] = (\n","        pd.to_numeric(X_test[col], errors='coerce')\n","          .replace([np.inf,-np.inf], np.nan)\n","          .fillna(-1).astype('int32')\n","    )\n","# (4) ëª¨ë¸ ì…ë ¥ìš© ë°ì´í„°\n","X_test_enc = X_test.drop(columns=id_cols)\n","cand = X_test[id_cols].copy()\n","\n","print('âœ… feature shapes â†’', X_test_enc.shape)\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â¹ Foldë³„ ëª¨ë¸ ë¶ˆëŸ¬ì™€ ì˜ˆì¸¡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","n = X_test_enc.shape[0]\n","preds_lgb = np.zeros(n)\n","preds_xgb = np.zeros(n)\n","preds_cat = np.zeros(n)\n","preds_mlp = np.zeros(n)\n","\n","for fold in (1,2,3):\n","    # LightGBM: Booster ê°ì²´ì´ë¯€ë¡œ predict_proba ëŒ€ì‹  predict\n","    m_lgb = joblib.load(f'{STACK_DIR}/lgb_fold{fold}.pkl')\n","    preds_lgb += m_lgb.predict(\n","        X_test_enc,\n","        num_iteration=m_lgb.best_iteration\n","    )\n","    # XGBoost: XGBClassifier ì´ë¯€ë¡œ predict_proba ê°€ëŠ¥\n","    m_xgb = joblib.load(f'{STACK_DIR}/xgb_fold{fold}.pkl')\n","    preds_xgb += m_xgb.predict_proba(X_test_enc)[:,1]\n","    # CatBoostClassifier\n","    m_cat = joblib.load(f'{STACK_DIR}/cat_fold{fold}.pkl')\n","    preds_cat += m_cat.predict_proba(X_test_enc)[:,1]\n","    # MLPClassifier\n","    m_mlp = joblib.load(f'{STACK_DIR}/mlp_fold{fold}.pkl')\n","    preds_mlp += m_mlp.predict_proba(X_test_enc)[:,1]\n","\n","# Foldë³„ í‰ê· \n","preds_lgb /= 3\n","preds_xgb /= 3\n","preds_cat /= 3\n","preds_mlp /= 3\n","\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ âº ëª¨ë¸ ë¸”ë Œë“œ & ì„ê³„ì¹˜ ì ìš© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","pred_test = (preds_lgb + preds_xgb + preds_cat + preds_mlp) / 4\n","cand['pred'] = pred_test\n","\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â» ì œì¶œíŒŒì¼ ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","rows = []\n","for oid, grp in cand.groupby('order_id'):\n","    prods = grp.loc[grp.pred >= best_thr, 'product_id'].astype(str).tolist()\n","    rows.append({\n","        'order_id': oid,\n","        'products' : ' '.join(prods) if prods else 'None'\n","    })\n","submission = pd.DataFrame(rows)\n","submission.to_csv(f'{OUT_DIR}/submission_blend.csv', index=False)\n","print('ğŸ“¤ submission saved â†’', f'{OUT_DIR}/submission_blend.csv')"],"metadata":{"id":"fJLt5A59t5Wt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from sklearn.metrics import f1_score\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 1. ì œì¶œ CSV ë¡œë“œ\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","sub = pd.read_csv(\n","    '/content/drive/MyDrive/instaCart_data/stack2/submission_blend.csv',\n","    dtype=str, keep_default_na=False\n",")\n","sub.columns = sub.columns.str.strip().str.lower()\n","if not {'order_id','products'}.issubset(sub.columns):\n","    raise ValueError(\"ì œì¶œ íŒŒì¼ì— 'order_id'ì™€ 'products' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n","\n","sub['order_id'] = sub['order_id'].str.strip()\n","def split_prod(cell: str) -> list[str]:\n","    cell = (cell or '').strip()\n","    return [] if cell.lower() in ('none','') else cell.split()\n","sub['products_pred'] = sub['products'].apply(split_prod)\n","sub = sub.drop_duplicates(subset=['order_id'])[['order_id','products_pred']]\n","print(f\"ì œì¶œ ì£¼ë¬¸ ìˆ˜: {len(sub):,}\")\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 2. GT ìƒì„±: order_products__train.csv ì—ì„œ ë§ˆì§€ë§‰ ì£¼ë¬¸ GT\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","train_lbl = pd.read_csv(\n","    '/content/drive/MyDrive/instaCart_data/order_products__train.csv',\n","    dtype={'order_id':str,'product_id':str}\n",")\n","gt = (\n","    train_lbl[train_lbl['reordered']==1]\n","      .groupby('order_id')['product_id']\n","      .apply(list)\n","      .reset_index()\n","      .rename(columns={'product_id':'products_gt'})\n",")\n","print(f\"GT ì£¼ë¬¸ ìˆ˜: {len(gt):,}\")\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 3. ê³µí†µ order_idë§Œ í‰ê°€\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","merged = pd.merge(gt, sub, on='order_id', how='inner')\n","print(f\"ê³µí†µ ì£¼ë¬¸ ìˆ˜: {len(merged):,}\")\n","if merged.empty:\n","    raise ValueError(\"GTì™€ ì œì¶œ íŒŒì¼ì˜ order_idê°€ í•˜ë‚˜ë„ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 4. ìƒí’ˆ vocabulary ìƒì„±\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","vocab = sorted({\n","    pid for lst in merged['products_gt']   for pid in lst\n","} | {\n","    pid for lst in merged['products_pred'] for pid in lst\n","})\n","print(f\"ìƒí’ˆ vocabulary í¬ê¸°: {len(vocab):,}\")\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 5. One-hot ì¸ì½”ë”©\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","mlb = MultiLabelBinarizer(classes=vocab)\n","y_true = mlb.fit_transform(merged['products_gt'])\n","y_pred = mlb.transform(   merged['products_pred'])\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 6. ì£¼ë¬¸ë³„ F1 â†’ í‰ê·  F1\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","kaggle_f1 = f1_score(\n","    y_true, y_pred,\n","    average='samples',\n","    zero_division=0\n",")\n","print(f\"\\nğŸ¯ Kaggle-style macro F1-score: {kaggle_f1:.6f}\")"],"metadata":{"id":"Kqp4y3Y6t_ib"},"execution_count":null,"outputs":[]}]}